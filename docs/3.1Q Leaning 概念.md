---
title: '3.1 Q-learning 概念'
description:
keywords:
---

# Q-learning 概念
我們每個人做事情都有自己的行為準則，這些準則通常是基於過去經驗和所獲得的回饋。例如，小時候爸媽常常會說「不寫完作業就不准看電視」。這樣的規範會讓我們明白，在「寫作業」這種狀態下，好的行為準則就是繼續寫作業，直到完成，因為這樣我們就可以得到獎勵——看電視。相反，如果我們選擇了不好的行為，比如沒有寫完作業就去看電視，那麼結果可能會是被父母發現，後果不堪設想。

![](https://i.imgur.com/w7JeF5w.png)

隨著這種經驗重複發生，這些情境和行為會逐漸深植在我們的記憶中，最終形成我們的行為準則。這種學習的過程與 Q-Learning 的概念有異曲同工之妙。

!!! note

    在 Q-Learning 中，系統透過不斷地在環境中探索（explore）和利用（exploit）已知的經驗來學習一個最優行為策略。類似於我們在小時候不斷學習該做什麼、不該做什麼，以便最終在生活中做出最好的選擇。

## Q-Learning 是什麼？
Q-Learning 是一種強化學習（Reinforcement Learning）的演算法，它的核心是透過決策過程來找出最優行為準則。我們可以透過一個具體的例子來說明 Q-Learning 是如何工作的。假設現在你處於一個「寫作業」的狀態，而在此之前，你並沒有嘗試過在寫作業的時候去看電視，因此對這兩種行為的後果並不清楚。這時，你有兩個選擇：

1. 繼續寫作業
2. 跑去看電視

由於你之前沒有因為沒寫完作業就看電視而被處罰過，所以你選擇了第二個行為——去看電視。當你選擇了看電視後，現在的狀態變成了「正在看電視」，你又面臨一次新的選擇：繼續看電視或做其他事情。

![](https://i.imgur.com/ZtJgeNk.png)

這樣的行為反覆發生，你一次又一次地選擇看電視，直到最後，爸媽回家發現你沒有完成作業便去看電視，於是你受到了嚴厲的懲罰。這一次的負面經歷讓你深刻記住了這個教訓，並在你的腦海中將「沒寫完作業就看電視」這一行為標記為負面的行為。


我們可以看到 Q-Learning 是如何根據過去經驗來進行決策的。假設我們已經有一張學習過的 Q 表。現在我處於狀態 𝑆1 ——正在寫作業，此時我有兩個選擇：

1. 𝐴1看電視
2. 𝐴2繼續寫作業

根據我過去的經驗，在狀態 𝑆1 下，選擇 𝐴2（寫作業）所帶來的潛在獎勵高於選擇 𝐴1看電視）。這可以用 Q 表來表示：

- 𝑄(𝑆1,𝐴1)=-2
- 𝑄(𝑆1,𝐴2)=1

由於 𝑄(𝑆1,𝐴2) 的值較高，我選擇 𝐴2——繼續寫作業。現在狀態更新為 𝑆2。在這個新狀態 𝑆2 中，我同樣有兩個選擇，並重複之前的過程：

- 𝑄(𝑆2,𝐴1)=-4
- 𝑄(𝑆2,𝐴2)=2

同樣，我會選擇 𝐴2 繼續寫作業。這樣，Q-Learning 通過不斷選擇最大潛在獎勵的行為來進行決策。

![](https://i.imgur.com/H8EWzXn.png)


## Q-Learning 更新
在 Q-Learning 的決策過程中，我們根據 Q 表的估計來進行行為選擇。首先，當我們處於狀態 𝑠1時，根據 Q 表的數值發現，行為 𝑎2的值比 𝑎1更高，因此在這種情況下，我們選擇了 𝑎2，進而轉移到新的狀態 𝑠2。

接下來，進入 Q 表的更新步驟。此時，我們並不會立即採取行為，而是會「想象」自己在 𝑠2 狀態下可能的行為結果，分別評估每種行為的 Q 值。假設在 𝑠2 狀態下，𝑎2 的 Q 值比 𝑎1 的 Q 值高，我們會將較高的 𝑄(𝑠2,𝑎2) 乘上一個衰減因子 𝛾 (例如 0.9)，再加上當前在 𝑠2 所獲得的獎勵 𝑅 (在此例中可能還沒有實際獲得獎勵，因此 𝑅=0)。

![](https://i.imgur.com/V8QvXKT.png)

這樣，根據我們在 𝑠2 的預測結果，便可以計算出更新後的現實 Q 值。而此時，我們會將這個現實 Q 值與之前依據 Q 表所估計的 Q 值進行比較，兩者的差距就是更新的依據。接著，我們將這個差距乘上一個學習率 𝛼，並加回到原來的 𝑄(𝑠1,𝑎2)，得到更新後的 Q 值。

![](https://i.imgur.com/YbePfTW.png)

!!! note

    雖然我們通過最大化 𝑄(𝑠2) 進行估算，但此時還沒有真正對 𝑠2 做出行為選擇，具體的行為選擇會在更新完 Q 表後再進行。這就是 Q-Learning 中「離線策略」(off-policy) 的特點。


## Q-Learning 演算法
以下虛擬碼總結了我們前面討論的Q-Learning算法的核心內容。在每次更新中，我們會使用「Q現實」和「Q估計」來進行更新。而Q-Learning的巧妙之處在於「Q現實」包含了對下一步最大的Q估計，這個估計經過衰減後與當前所獲得的獎勵結合，來形成這一步的「Q現實」。

![](https://i.imgur.com/pEdCUsD.png)

以下是演算法中一些重要的參數：

- **ε-greedy 策略**：這是一種在決策時用到的探索策略，當ε為0.9時，表示有90%的機會我們會根據Q表中的最優行為來選擇，而10%的時間則會隨機探索其他行為。
- **α（學習率）**：這個參數用來控制每次學習的速度，決定這次更新時有多少誤差會被納入學習。它的值在0到1之間。
- **γ（衰減因子）**：這個參數決定了對未來獎勵的重視程度。當γ等於1時，代表我們能夠清楚地看到所有未來的獎勵，當γ等於0時，則表示我們只關心當前的獎勵，對未來沒有考慮。

![](https://i.imgur.com/FTrH6Qz.png)

可以這樣理解，𝛾 等於 1 時，代表機器人有一副「完美的眼鏡」，它能夠清晰地看到未來所有的獎勵變化。但如果 𝛾趨近於 0，機器人則會「近視」，它只能看到眼前最近的獎勵，無法顧及未來的價值。隨著 
𝛾 從 0 漸漸增大，機器人的「視力」也會逐漸改善，它會慢慢考慮更遠的未來獎勵，不再只關注眼前的利益。

!!! note

    Q-Learning 的這一算法讓機器人在選擇行動時不僅僅考慮眼前的收益，還能將未來的潛在收益一併考慮進來，隨著不斷更新Q表，決策變得越來越明智。
