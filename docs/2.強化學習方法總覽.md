
## Model-Free vs Model-Based
1. **Model-Free 方法**：

代理人無需理解環境，僅通過從環境中獲取的反饋進行學習。
- 優點：計算量通常較小，實現簡單。
- 缺點：需要在真實環境中進行大量的探索和試錯，學習效率相對較低。

2. **Model-Based 方法**：

代理人通過理解並建模環境，能夠在虛擬環境中模擬行為結果。
- 優點：能夠預測行為結果，減少試錯成本，提升學習效率。
- 缺點：需要額外的計算資源來構建和維護環境模型。

![](https://i.imgur.com/8h4Mkk6.png)

Model-Based 方法還有一個致勝法寶——想像力。透過建立環境模型，代理人能夠預測未來的狀況，進行計劃和決策。而 Model-Free 方法中，代理人只能等待每次行為的實際反饋，然後再做出下一步行動。這就是為什麼 AlphaGo 能夠超越人類，在圍棋賽場上取得勝利，因為它能夠通過預測無數未來的可能性來選擇最佳行動。

### Model-Free 方法
如果我們不去試圖理解環境，而是單純根據環境給我們的反饋來學習，那麼這就是 Model-Free 方法。這類方法並不試圖建立環境的模型，代理人僅僅通過與環境的互動，從反饋中學習如何達到目標。簡單來說，代理人不知道環境的內部運作，只能根據每次行為得到的結果進行學習和調整。

**例子**：
- Q-Learning：通過狀態-行為對的價值估算來決定最佳行動。
- Sarsa：類似 Q-Learning，但基於代理人在下一個狀態中的行為來更新價值。
- Policy Gradients：通過直接學習策略，來決定如何在每個狀態下選擇行動。

> 這些方法都不考慮環境的具體結構，僅通過直接與環境互動來學習。

### Model-Based 方法
相比之下，Model-Based 方法則多了一道重要的步驟：它會嘗試建立一個能夠代表環境的模型，這個模型能夠預測環境如何對代理人的行為做出反應。換句話說，代理人不僅在真實環境中學習，還可以通過這個模型來模擬環境中的行為，提前預測某些行動可能產生的後果，從而減少在真實環境中的錯誤。

舉個例子，假設我們有一個機器人在現實世界中玩耍。這個機器人如果不理解環境，可能會隨意丟出一顆原子彈，結果把自己炸死了。這就是 Model-Free 的情況——機器人直接與環境互動，並根據實際結果學習。而 Model-Based 方法則不同，機器人會根據以往的經驗學習如何建立一個環境模型，並在這個虛擬環境中模擬出它的行為結果。這樣，它可以在模擬的環境中“玩耍”，比如模擬丟出原子彈的情境，避免在真實世界中犯錯。

![](https://i.imgur.com/Mxs784y.png)

總結來說，Model-Free 方法更直接，通過與真實環境的互動學習；而 Model-Based 方法則強調通過構建環境模型，進行預測與學習。這兩種方法各有優缺點，適用於不同的應用場景。


## Policy-Based 和 Value-Based
在強化學習中，除了以是否理解環境來區分方法之外，還可以根據決策的方式將方法分為兩類：**Policy-Based（基於策略）** 和 **Value-Based（基於價值）** 方法。這兩種方法各有其優勢與應用場景，以下將對這兩種方法進行詳細介紹。

- **Policy-Based（基於策略）** 方法通過動作概率來決策，能夠很好地處理連續動作空間的問題，具有更高的靈活性。
- **Value-Based（基於價值）** 方法基於動作價值來決策，決策果斷，但在處理連續動作時存在限制。
- **Actor-Critic（結合兩者優勢）** 結合了兩者的優勢，透過同時考慮策略和價值，實現了更高效的學習。

> 這兩類方法在強化學習中各有應用，且它們之間的結合方法（如 Actor-Critic）使得強化學習在複雜環境中的表現更加優秀。

### Policy-Based（基於策略）
Policy-Based 方法是強化學習中最直接的一種方法，它根據環境的當前狀態，通過策略函數直接輸出下一步各種行動的概率。每個可能的動作都會有一個對應的概率，代理人根據這個概率來決定選擇哪一個行動。因此，即使某個動作的概率最高，也不一定會被選擇，因為代理人是依據概率進行隨機選擇。

![](https://datawhalechina.github.io/easy-rl/img/ch1/1.31.png)

這種方法的優點之一是能夠很好地處理連續動作空間。在連續動作的場景中，基於價值的方法無法輕易處理，因為它需要為每一個可能的行動都計算價值，而連續動作空間無窮無盡。然而，Policy-Based 方法通過生成一個概率分佈，就可以在連續動作中選擇特定的行動，這讓它在處理複雜動作的問題上更具靈活性。

**例子**：
- Policy Gradients：這是典型的 Policy-Based 方法，通過優化策略來提高高回報行為的選擇概率。

### Value-Based（基於價值）
相較於基於策略的方法，Value-Based 方法則是根據每個可能動作的價值進行選擇。代理人在決策時，會計算出每一個行動的價值，然後選擇價值最高的行動。這使得 Value-Based 方法的決策更為「果斷」，因為它總是選擇當前價值最高的行為，並不會考慮隨機性。

![](https://datawhalechina.github.io/easy-rl/img/ch1/1.32.png)

這種方法的限制在於，當面對連續動作空間時，計算每一個動作的價值變得困難且不可行。Value-Based 方法適合於離散動作空間，能夠有效地對每一個具體的動作進行價值估計。

**例子**：
- Q-Learning：透過學習每個狀態-行動對的價值來決定最優行為。
- Sarsa：類似 Q-Learning，但其更新是基於代理人在下一個狀態中實際選擇的行動。

### Actor-Critic（結合兩者優勢） 
既然 Policy-Based 和 Value-Based 各有其優勢與劣勢，研究者們發展出一種結合兩者的方法，稱為 Actor-Critic。這種方法中的 Actor 負責基於策略輸出行動的概率分佈，而 Critic 則負責根據行動的價值對選擇進行評價。

Actor-Critic 的好處在於，它能夠利用 Policy-Based 方法靈活的行動選擇，並通過 Value-Based 方法加速學習過程。Critic 提供的價值函數能夠幫助 Actor 更快速地學習如何優化策略，從而提升學習效率。

**例子**：
- Actor-Critic：將策略梯度與價值估計結合的強化學習算法，是目前廣泛使用的一類方法。

## Episode-based 和 Step-based
在強化學習中，我們可以根據更新策略的時機將方法分為兩大類：**Episode-based（回合更新）** 和 **Step-based（單步更新）**。這兩種方式在如何更新代理人的行為準則上有著本質的不同。


![](https://i.imgur.com/NwHi37I.png)

1. **Episode-based（回合更新）**：

- 代理人在回合結束後總結整個過程，根據最終結果來更新策略。
- 適合有明確起點與終點的任務，例如一局圍棋、一場遊戲或任務結束時才得到回報的情境。
- 優點：可以看到整體回報，避免過早更新錯誤策略。
- 缺點：學習速度較慢，無法及時調整策略。

2. **Step-based（單步更新）**：

- 代理人在每一步行動後根據當前結果進行更新，能夠即時學習。
- 適合持續動態的問題或需要快速適應的場景，例如即時決策或控制任務。
- 優點：學習速度快，能夠及時調整行為。
- 缺點：可能會過於依賴短期結果，導致策略不夠穩定。

![](https://i.imgur.com/aMXL5TJ.png)

回合更新適合於那些需要觀察整體結果的場景，而單步更新則更適合需要即時反應的情境。在現代強化學習中，由於單步更新的高效率，大多數的強化學習算法都基於此方法。例如，Q-Learning 和 Sarsa 就是典型的單步更新方法。

### Episode-based（回合更新）
回合更新指的是代理人在一個完整的遊戲回合（Episode）結束後，才進行學習和策略更新。也就是說，代理人在每次遊戲開始後，會持續地進行行動，直到達到遊戲的結束點。等回合結束後，代理人總結這一回合中所有的行動和轉折點，然後再更新策略和行為準則。

這種方法的特點在於，它將學習過程延遲到回合結束後，因此在回合進行中，代理人無法即時調整策略。這類方法比較適合那些有明確開始和結束界限的問題，例如遊戲中的一局棋局或模擬中的一個完整任務。

**例子**：
- Monte Carlo Learning：代理人在每個回合結束後，根據回合中的累積回報來更新其策略。
- 基礎版的 Policy Gradients：策略梯度方法中，也有部分算法是基於回合更新的方式來優化策略。

### Step-based（單步更新）
與回合更新不同，單步更新是指代理人在遊戲進行中的每一步都會根據當前的狀態和回報立即進行策略更新。代理人不必等待遊戲的結束，而是能夠在每次行動後立刻學習，並在下一次行動中根據更新後的策略進行選擇。

單步更新的好處在於，它能夠邊玩邊學習，讓代理人及時調整行為，從而加快學習速度。在許多實際應用中，單步更新的效率更高，因此被廣泛採用。

**例子**：
- Q-Learning：Q-Learning 是一種典型的單步更新方法，代理人根據每一步的狀態轉移和回報，及時更新 Q 值。
- Sarsa：類似於 Q-Learning，Sarsa 也是一種逐步更新的價值學習方法，根據每個狀態-行動對的結果來更新策略。
升級版的 Policy Gradients：在策略梯度方法中，進一步的改進也導入了單步更新的概念，使得學習效率提升。
