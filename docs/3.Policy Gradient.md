# DRL Lecture 1: Policy Gradient
這篇文章將介紹一個強化學習中的進階技術，名為 Proximal Policy Optimization (PPO)。PPO 是一種政策梯度的升級版方法，被 OpenAI 當作他們的默認強化學習演算法。假如你要實作強化學習，PPO 很可能是你首先會考慮的算法之一。

我們將先快速回顧 Policy Gradient，讓大家再次熟悉這個基礎概念，隨後會深入探討如何從 on-policy 方法轉變為 off-policy 方法，並進一步解釋這兩者的區別。最後，我們將介紹如何在 off-policy 方法上加入一些約束條件，進而形成 PPO 演算法。

![](https://i.imgur.com/NSKoS8K.png)

## 回顧 Policy Gradient
首先，我們來快速回顧一下 Policy Gradient，因為 PPO 是 Policy Gradient 的變形，因此理解這個基礎非常重要。在強化學習（Reinforcement Learning, RL）中，主要有三個基本組件：

- Actor：即代理人，負責決策，選擇下一步應該執行的行動。
- Environment（環境）：環境決定代理人的行為會產生什麼樣的結果，並提供當前的狀態。
- Reward Function（回報函數）：根據代理人的行動，提供一個回報，衡量行動的好壞。

在本章節的實作練習中，目標是讓機器學會玩電子遊戲。此時，Actor 的角色就是操控遊戲中的搖桿，比如控制角色向左、向右或開火等動作。Environment 就是遊戲主機，它負責控制遊戲的畫面、怪物的移動以及顯示當前的狀態。而 Reward Function 則決定了當你執行某些行動時，會得到多少分數。比如，每當你擊敗一個怪物時，就可以獲得 20 分。

![](https://i.imgur.com/YmlU0Zf.png)

同樣的概念也適用於圍棋中。在這裡，Actor 就是 AlphaGo，負責決定每一步棋的落子位置。Environment 是對手，負責回應每次的行動。而 Reward Function 則根據圍棋規則決定，贏得比賽你會得到 1 分，輸掉則得到 -1 分。

在強化學習中，Environment 和 Reward Function 是事先給定的，也就是說，它們在學習開始前就已經確定，並且無法被控制或修改。我們唯一可以控制和調整的是 Actor，也就是代理人決策的 Policy（策略）。我們的目標是調整 Actor 的策略，使其能在給定的環境和回報函數下獲得最大的回報。

## Policy of Actor
在強化學習中，Actor（執行者）負責決定在當前狀態下要採取的行為，而這個決策過程是由一個 Policy（策略）來控制的。Policy 𝜋 是一個具有參數 𝜃 的神經網路，其輸入和輸出決定了 Actor 如何選擇行為。

- **Policy 的結構**：
    - **輸入（Input）**：這是機器觀察到的狀態，通常表示為向量或矩陣。如果是在玩電子遊戲，輸入就是遊戲的畫面，這些畫面通常是由像素（pixels）所組成。
    - **輸出（Output）**：每個可能的行為對應於輸出層中的一個神經元。比如在遊戲中，Actor 可能有三個可選的行為，分別是向左移動、向右移動和開火，這三個行為分別對應輸出層的三個神經元。

!!! note

    在使用**深度學習**技術來進行強化學習時，網路的參數 𝜃 會根據學習過程不斷調整，使 Policy 能夠在不同狀態下做出更優化的決策。

當網路接收到輸入後，它會根據當前狀態對每個行為分配一個分數，這些分數代表了採取該行為的機率分布。Actor 會根據這個機率分布來決定下一步的行為。例如，當輸出的機率分布為 70% 向左、20% 向右、10% 開火時，Actor 就有 70% 的機率選擇向左，20% 的機率選擇向右，10% 的機率選擇開火。

![](https://i.imgur.com/8Q2bDTj.png)

這樣的機率分布決定了 Actor 在不同情況下所採取的行為，並且根據學習過程的進行，這些機率會不斷調整，從而提升 Actor 的策略。

### Example: Playing Video Game
接下來我們來介紹 Policy 的部分，它實際上是一個神經網路。現在，我們用一個具體的例子快速說明 Actor 如何與環境進行互動。

首先，Actor 會看到遊戲的初始畫面，這個畫面我們用 𝑆1 來表示。當 Actor 看到這個初始畫面後，根據其內部的神經網路和策略（Policy），它會決定採取一個行動。例如，Actor 可能決定向右移動。決定行動後，Actor 就會根據該行動獲得一個回報（Reward），代表這個行動得到多少分數。

在這個過程中，我們可以用 𝑆1 表示初始狀態，用 𝐴1 來表示第一次執行的動作，並用 𝑅1 來表示這次行動後得到的回報。有些文獻中可能會用不同的符號，例如 𝑅2 來表示回報，這都可以，最重要的是你自己理解這個過程。

![](https://i.imgur.com/idYxfeP.png)

接下來，Actor 會看到新的遊戲畫面（我們用 𝑆2 來表示），然後 Actor 再次根據新的畫面決定下一個行動，比如開火。如果這個行動成功殺掉了一個怪物，它可能會獲得 5 分的回報。這個過程會不斷反覆進行。

直到某一時刻，當 Actor 執行了一個動作並獲得回報後，環境決定遊戲結束。例如，在某些遊戲中，Actor 控制一艘綠色的船殺怪物，如果船被殺死，遊戲就結束；或者當所有怪物都被消滅時，遊戲也會結束。

![](https://i.imgur.com/av9zul2.png)

整個遊戲過程稱為一個 episode，而在這個 episode 中所累積的所有回報總和就是 total reward，我們可以用 𝑅 來表示。Actor 存在的最終目的是最大化（maximize）它所能獲得的總回報（reward）。

### Actor, Environment, Reward 關係
我們可以用圖像化的方式來解釋 Environment（環境）、Actor（執行者）和 Reward（回報）之間的關係。首先，環境其實可以看作是一個函數，雖然遊戲主機內部可能不是神經網路，而是基於規則的系統，但我們仍然可以把它當作一個函數來理解。

這個函數一開始會給出一個初始狀態（例如遊戲的畫面），讓 Actor 進行觀察。當 Actor 看到這個初始畫面 𝑆1 後，根據策略決定並執行一個行動 𝐴1 。接著，環境會將這個行動 𝐴1 作為輸入，並返回下一個狀態 𝑆2 也就是新的遊戲畫面）。

![](https://i.imgur.com/L8kaf9D.png)  

Actor 再次根據這個新的畫面 𝑆2決定下一個行動 𝐴2，環境接收這個行動後，繼續返回新的狀態 𝑆3。這個過程會不斷重複，直到環境判斷遊戲應該結束為止。

在一場遊戲中，我們可以將 Environment（環境）輸出的狀態 𝑆 與 Actor（執行者）輸出的行為 𝐴 組合在一起，形成一條軌跡，這條軌跡稱為 Trajectory。每一個 Trajectory 都可以計算它發生的機率。如果我們假設 Actor 的參數 𝜃 已經設定好，那麼就可以根據這些設定來計算某個 Trajectory 出現的機率，也就是在一個回合或一個 Episode 中發生這樣情況的概率。

![](https://i.imgur.com/TJOSgId.png)

如何計算這個機率呢？我們可以按照以下步驟進行：

1. 首先，計算環境輸出初始狀態 𝑆1 的機率 𝑝(𝑆1)。
2. 然後，根據狀態 𝑆1 決定行為 𝐴1 的機率，這個機率是由 Actor 的策略（Policy）決定，並且依賴於網路的參數 𝜃，表示為 𝑝𝜃(𝐴1∣𝑆1)。這裡我們之前提到過，Actor 的輸出其實是一個機率分佈，根據這個分佈來選擇具體的行動。

![](https://i.imgur.com/wIgvjxJ.png)

接下來，環境根據 Actor 執行的行為 𝐴1 和當前狀態 𝑆1 輸出下一個狀態 𝑆2。這個過程也可能具有機率性，取決於環境本身的設定。如果環境是完全決定性的，那每次相同的行為和狀態都會導致相同的下一個狀態；但若環境內部帶有隨機性，那麼每次產生的下一個狀態可能會不同。

![](https://i.imgur.com/uLv2a5l.png)

這個過程會不斷重複下去，直到遊戲結束。根據這個過程，我們可以計算出整個 trajectory 的機率，這取決於兩個部分：

1. **環境的行為**：即環境如何根據前一個狀態 𝑆𝑡 和行為 𝐴𝑡 生成下一個狀態 𝑆𝑡+1。這部分通常是由環境的內部設計決定的，並且我們無法控制。
2. **Actor 的行為**：即 Actor 根據當前狀態 𝑆𝑡 選擇行為 𝐴𝑡 的機率。這由 Actor 的參數 𝜃 控制，是我們可以通過學習進行優化的部分。

!!! note
    隨著 Actor 的行為不同，每一個 trajectory 出現的機率也會有所不同。因此，我們的目標是調整 Actor 的參數，最大化 trajectory 中獲得的回報（reward）。

在強化學習中，除了環境（Environment）和執行者（Actor）之外，還有一個重要的角色叫做 Reward Function（回報函數）。Reward Function 的作用是根據某個狀態（State）下採取的某個行為（Action）來決定該行為能夠獲得多少分數，它是一個函數。舉例來說，當你給它狀態 𝑆1 和行為 𝐴1 時，Reward Function 會告訴你得到的回報是 𝑅1。同樣地，給它 𝑆2 和 𝐴2 ，它會返回 𝑅2。

![](https://i.imgur.com/faNUogq.png)

我們把所有小的回報 𝑟𝑡 加總起來，就得到了一個 總回報 𝑅。這裡的 𝑅 代表某一個 Trajectory（軌跡）在一場遊戲或一個回合（Episode）中累積的總回報。在強化學習中，我們的目標是調整 Actor 內部的參數 𝜃，使得總回報 𝑅 越大越好。然而，實際上 Reward 並不是一個固定的值，而是一個隨機變數。這是因為：

1. Actor 在相同狀態下選擇行為時具有隨機性。
2. Environment 在相同行為下生成觀察值（Observation）時也可能具有隨機性。

因此，總回報 𝑅 是一個隨機變數，而我們能計算的是它的期望值（Expected Reward）。這個期望值代表了在給定某組參數 𝜃 的情況下，能夠期望獲得的總回報。

我們如何計算這個期望值呢？具體來說，首先要列舉出所有可能的 Trajectory，每個 Trajectory 都有一個對應的機率，這個機率是由 Actor 的參數 𝜃 決定的。假如 Actor 的參數非常優化，且模型表現很好，那麼長壽命、高回報的 Episode 會有較高的機率出現，而短命、低回報的 Episode 機率會較低。

根據這些機率，我們可以計算每個 Trajectory 的總回報，然後對所有可能的 Trajectory 進行加權總和，這就是總回報的期望值。我們可以將這個過程寫作：

![](https://i.imgur.com/qVJdX8K.png)

這表示從 𝑝𝜃(𝜏) 的分佈中抽樣一個 Trajectory，然後計算其總回報 𝑅(𝜏) 的期望值。