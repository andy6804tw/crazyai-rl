
## Q-Learning
Q-Learning 是一種基於值的學習方法 (Value-Based Method)，它關注的是學習狀態-行為的值，而非直接學習策略 (Policy)。 透過這種方法，模型不會直接決策，而是通過 Critic 來評估當前行為的好壞。

![](https://i.imgur.com/7N08GJJ.png)

**Critic 的功能**：

- Critic 的工作是評估一個已經存在的 Actor（即決策者）的表現，而不是直接採取行為。具體來說，Critic 會計算 State Value Function 𝑉𝜋(𝑠)，該函數表示當前狀態 𝑠 下，Actor 𝜋 在這個狀態之後能夠累積多少預期獎勵。
- Critic 的輸出是依賴於當前的 Actor，並會根據 Actor 與環境的互動來評估後續可以累積多少的獎勵。

!!! note

        Critic 是與 Actor 綁定的，其主要功能是評估 Actor 的決策表現，但不會直接做出決策。Critic 的輸出結果反映了 Actor 在特定狀態下的預期表現。

舉個例子，在遊戲 Space Invader 中，當畫面上還有很多敵人時，Actor 有很多機會消滅敵人，獲得高分，這時候 Critic 的評價 𝑉𝜋(𝑠) 會很高。相反，如果敵人所剩不多，或遊戲狀況不利，Critic 的評價值會較低，因為接下來獲得獎勵的機會有限。Critic 的評價依賴於當前的 Actor，如果 Actor 很弱，即便是相同的狀態，累積的獎勵也會較低。這強調了 Critic 的輸出不僅依賴於狀態，還取決於 Actor 的表現，兩者是密切相關的。

![](https://i.imgur.com/2rMldG0.png)

## 如何估計狀態價值函數？
在估計狀態價值函數 𝑉𝜋(𝑆)時，有兩種常見的方法：Monte-Carlo（MC）方法和時間差分（Temporal Difference, TD）方法。


### 蒙地卡羅方法 MC-Based
Monte-Carlo方法是一種直觀的做法，它的核心概念是讓Actor 𝜋 與環境互動，然後Critic觀察並統計從某一個狀態開始，直到整個遊戲結束後的累積回報。例如，當Actor 𝜋 在狀態 𝑆𝑎 時，累積的獎勵可能是 𝐺𝑎，在狀態 𝑆𝑏 時累積的獎勵可能是 𝐺𝑏。這樣，我們可以訓練一個網路來預測不同狀態下的累積獎勵值，這實際上就是一個迴歸問題，我們希望網路輸出與真實累積獎勵越接近越好。這種方法需要等到每一個遊戲回合結束後才能進行更新，因此在處理長遊戲回合時，可能會耗費大量時間。

![](https://i.imgur.com/IX4W1Ka.png)

??? tip "簡單的例子來解釋 MC 方法"

    #### Monte Carlo 方法的例子
    假設你正在玩一個桌上遊戲，每回合結束時可以獲得一個分數。我們要估計在每一回合的不同狀態下能夠得到的累積分數。Monte Carlo 方法的核心思想是：你要等到整個遊戲（回合）結束後，才能計算這回合的最終分數，然後將這個分數反映到所有經歷過的狀態中。

    ##### 假設遊戲過程如下：
    - **起點（State A）** → 行動 → **中間狀態（State B）** → 行動 → **終點（State C）**
    - 這場遊戲的最終分數是 **5 分**。

    ##### Monte Carlo 方法具體步驟：
    1. **在 State A**：
    - 你採取了某個行動，進入了 **State B**。
    2. **在 State B**：
    - 你採取了另一個行動，進入了 **State C**，並最終完成了這場遊戲。
    3. **遊戲結束後**，你知道這場遊戲最終獲得的分數是 **5 分**，這個就是累積的獎勵。

    ##### MC 如何計算價值：
    - **State A 的價值**：因為你從 State A 開始，最終累積的獎勵是 **5 分**，所以 Monte Carlo 方法會將這 **5 分**作為 **State A** 的價值更新。
    - **State B 的價值**：同理，雖然你是在 State B 過程中，但你同樣會將遊戲結束後的總獎勵 **5 分**更新到 **State B**。

    #### 總結：
    Monte Carlo 方法的特點在於**遊戲結束後才進行更新**，不管遊戲中間經歷了哪些狀態或採取了什麼行動，你在每一個狀態下所獲得的價值，都是根據這場遊戲的**最終累積獎勵**來進行更新的。

    在這個例子中：

    - **State A** 和 **State B** 都會更新為 **5 分**，因為整場遊戲的最終獎勵是 **5 分**。

    這種方法的優點是直觀且簡單，但缺點是必須等到遊戲結束才能更新，這對於那些遊戲回合非常長的情況可能不是很高效，這也是為什麼 Temporal Difference (TD) 方法會在這裡派上用場，因為 TD 不需要等遊戲結束就可以進行更新。


### 時間差分方法 TD-based
相對的，時間差分（TD）方法則不需要等待遊戲結束就可以進行更新。在這種方法中，當Actor 𝜋 在某個狀態 𝑆𝑡 行一個動作 𝐴𝑡 後，會立即獲得一個即時獎勵 𝑟𝑡，並轉移到下一個狀態 𝑆𝑡+1。根據這個轉移，我們可以直接根據當前狀態的價值和下一個狀態的價值來進行更新。具體來說，TD方法認為當前狀態 𝑆𝑡 的價值應該等於即時獎勵 𝑟𝑡 加上下一個狀態 𝑆𝑡+1 的價值。訓練時，網路會根據這種差異進行更新，並不需要等到整個遊戲結束。這使得TD方法能夠更快地進行更新，特別適合處理長期回合的遊戲。

![](https://i.imgur.com/3duepvH.png)

在進行訓練時，我們並不是直接估算價值函數 𝑉，而是希望透過學習使其符合特定的關係式。具體來說，我們將當前狀態 𝑆𝑡 丟入網路中，得到 𝑉(𝑆𝑡)，然後將下一個狀態 𝑆𝑡+1 也丟入網路，得到 𝑉(𝑆𝑡+1)。根據這個關係式， 𝑉(𝑆𝑡)−𝑉(𝑆𝑡+1) 應該等於當前步驟的獎勵 𝑅𝑡。因此，我們會設定一個損失函數，讓這兩者的差異（即 𝑉(𝑆𝑡)−𝑉(𝑆𝑡+1) 和 𝑅𝑡 的差距）越小越好。隨著這樣的訓練過程，我們可以逐步優化網路的參數，讓網路更準確地學習到這個價值函數 𝑉。

??? tip "簡單的例子來解釋 TD 方法"

    假設我們在玩一個迷宮遊戲，每次我們做出一個動作（例如向前走一步），就會獲得即時獎勵 𝑟𝑡，這個獎勵可能是正的（我們走對了方向），也可能是負的（我們走錯了方向）。同時，我們會從一個狀態 𝑆𝑡 轉移到下一個狀態 𝑆𝑡+1。

    TD 方法的核心思想是，當你在 𝑆𝑡 的時候，你可以估計該狀態的價值 𝑉𝜋(𝑆𝑡)，但你不需要等到整個遊戲結束。你可以馬上利用你轉移到的下一個狀態 𝑆𝑡+1 的價值來更新當前狀態 𝑆𝑡 的估計值。

    例如，當你從 𝑆𝑡 移動到 𝑆𝑡+1，你會得到一個即時獎勵 𝑟𝑡。這時，你可以說：𝑆𝑡 的價值應該是 𝑆𝑡+1 的價值加上你剛剛得到的獎勵 𝑟𝑡，這就是公式：

    $$ V^{\pi}(S_t) = V^{\pi}(S_{t+1}) + r_t $$


    換句話說，當前的狀態 𝑆𝑡 價值，應該是從 𝑆𝑡+1 繼續走下去的價值，加上你剛剛獲得的這個獎勵。這樣，我們可以不必等到遊戲結束就可以更新狀態的價值，並且根據每一次的轉移，逐步學習狀態的價值。

    例如，假設你在迷宮的某一點 𝑆𝑡 做出一個決策後馬上向正確的方向前進並獲得了 𝑟𝑡=+10 的獎勵，而接下來的狀態 𝑆𝑡+1 又很接近出口，它的價值很高（假設 𝑉𝜋(𝑆𝑡+1)=50）。那麼，當前的狀態 𝑆𝑡 的價值就可以更新為：

    𝑉𝜋(𝑆𝑡)=50+10=60

    這樣你可以更快速地學到該狀態 𝑆𝑡 的價值，而不必等到遊戲結束。 TD 方法的好處在於它可以在遊戲過程中逐步更新價值估計，而不需要等到整個遊戲回合結束。


??? tip "補充說明"

    當我們提到價值 𝑉𝜋(𝑆𝑡) 時，它考慮的是從 狀態 𝑆𝑡 開始，根據策略 𝜋 所能預期獲得的未來所有步驟的累積獎勵。

    具體來說，這個累積獎勵包括從 𝑆𝑡 開始的 當前獎勵 𝑟𝑡，以及接下來從 𝑆𝑡+1、𝑆𝑡+2 等未來各個狀態所獲得的所有獎勵。也就是說，它不僅僅指下一個狀態 𝑆𝑡+1 的獎勵，而是 從 𝑆𝑡 開始到最終結束時的所有獎勵的期望值。

    我們可以用這個公式來表達：

    $$ 
    V^\pi(S_t) = \mathbb{E}_\pi [ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots ]
    $$

    這裡，𝑟𝑡 是當前的即時獎勵，𝛾 是折扣因子（通常小於 1），用來減少未來獎勵的影響。所以，價值 𝑉𝜋(𝑆𝑡) 是從當前狀態 𝑆𝑡 開始，依據策略 𝜋 所能預期獲得的所有未來獎勵的加權和。

    實際上，公式 γrt+1+γ2rt+2+⋯ 可以等同於 𝑉𝜋(𝑆𝑡+1) ，因為它代表的是從狀態 𝑆𝑡+1 開始，未來所有步驟的累積折扣獎勵。因此，我們可以將未來的累積獎勵這部分總結為：

    $$ V^{\pi}(S_t) = r_t + \gamma V^{\pi}(S_{t+1}) $$

    這裡，𝑟 𝑡是在當前狀態 𝑆𝑡 獲得的即時獎勵，而 𝑉𝜋(𝑆𝑡+1) 是從狀態 𝑆𝑡+1 開始，根據策略 𝜋 所能預期獲得的累積獎勵（包括從 𝑆𝑡+1 開始的即時獎勵及其後續獎勵）。這個關係式揭示了 Temporal-Difference (TD) 的核心概念，即我們可以通過**當前獎勵** 𝑟𝑡 和**下一個狀態的價值** 𝑉𝜋(𝑆𝑡+1) 來估算當前狀態 𝑆𝑡 的價值 𝑉𝜋(𝑆𝑡)。


### 比較 MC 和 TD 差異
MC 和 TD 各有優劣。MC 方法直覺簡單，基於整場遊戲的累積獎勵來更新，但其變異性較大，且需要完整的遊戲回合數據。TD 方法則是基於當前步驟的獎勵和下一步的估計來更新，變異性較小且能即時更新，但可能會有偏差。TD 通常在較長、較複雜的情境中更有效率，而 MC 則適合簡單、回合較短的問題。

| **比較項目**          | **Monte Carlo (MC)**                            | **Temporal Difference (TD)**                     |
|----------------------|-------------------------------------------------|-------------------------------------------------|
| **更新時間**         | 需要等到遊戲結束後，才能累積更新值              | 可以在每一步更新                               |
| **數據利用率**       | 只在遊戲結束時利用全部數據                      | 可以在每次狀態轉移後立即更新                     |
| **Variance (變異性)** | 變異性較大，因為累積了整場遊戲的隨機變量，結果可能波動大         | 變異較小，因為每一步的更新只依賴於當前觀察到的獎勵           |
| **Bias (偏差)**      | 偏差較小，因為使用最終的累積獎勵，較能反映真實值                | 可能會有較大的偏差，因為依賴於未來的估計值，而非最終結果     |
| **需求的經驗**       | 需要完整的遊戲回合數據，無法即時更新             | 可以逐步學習，即時更新，無需整個遊戲回合       |
| **收斂速度**         | 通常較慢，因為需要等待整個回合結束才能更新         | 通常較快，因為每步驟都會更新                   |
| **適用情境**         | 適合情境簡單、遊戲長度較短的問題                 | 適合情境較為複雜、遊戲長度較長的問題           |

MC 和 TD 各有優劣。MC 方法直覺簡單，基於整場遊戲的累積獎勵來更新，但其變異性較大，且需要完整的遊戲回合數據。TD 方法則是基於當前步驟的獎勵和下一步的估計來更新，變異性較小且能即時更新，但可能會有偏差。TD 通常在較長、較複雜的情境中更有效率，而 MC 則適合簡單、回合較短的問題。


!!! note

        MC方法更適合完整回合的回報計算，而TD方法則更具即時性和效率。兩者的選擇依賴於應用場景和遊戲的長度。


從以下例子可以發現，即便是用相同的資料集，MC、TD兩種方法所估測出來的結果是不一樣的。例子中有 8 個 episode，每個 episode 包含兩個狀態 𝑠𝑎 和 𝑠𝑏，以及相應的獎勵 𝑟。

![](https://i.imgur.com/rSJB8X5.png)

1. **MC 方法**：

    Monte Carlo 方法在計算 \(V^\pi(s_a)\) 的時候，使用完整 episode 的最終累積獎勵。由於狀態 \(s_a\) 在每次的 episode 結束時都得到的累積獎勵是 0，因此 Monte Carlo 給出的 \(V^\pi(s_a)\) 是 0。

2. **TD 方法**：

    Temporal Difference 方法則更靈活，利用當前的獎勵和下一個狀態的估計來更新價值函數。根據投影片中的例子，狀態 \(s_b\) 的價值已經被計算為 3/4，因此 TD 使用這個值來計算 \(V^\pi(s_a)\)，即 \(V^\pi(s_a) = V^\pi(s_b) + r = 3/4 + 0 = 3/4\)。


##  Q-function
接下來我們要介紹另一種 Critic，稱為 Q-function，也叫做 State-action value function。在我們之前討論的 State value function 中，輸入僅為一個狀態（state），根據該狀態預測從該點開始的累積獎勵期望值（expected cumulative reward）。然而，在 Q-function 中，輸入變成了一對 State 和 Action（狀態與行動的組合），目的是估計在某個狀態 𝑠 下，強制執行某個行動 𝑎，然後讓 Actor 𝜋 繼續操作下去後，最終能獲得的累積獎勵的期望值。